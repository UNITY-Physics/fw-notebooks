{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "from nilearn import image, plotting\n",
    "import nibabel.freesurfer as fs\n",
    "from nilearn.image import resample_to_img\n",
    "import matplotlib.colors as mcolors\n",
    "from nibabel.freesurfer import read_annot\n",
    "\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.image as mpimg\n",
    "import re\n",
    "import datetime\n",
    "import os\n",
    "#from dotenv import load_dotenv\n",
    "import flywheel\n",
    "\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.lib.utils import ImageReader\n",
    "\n",
    "today = datetime.datetime.now()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Source zshrc and get the env var you want\n",
    "def get_env_from_zshrc(var_name):\n",
    "    command = f\"source ~/.zshrc && echo ${var_name}\"\n",
    "    result = subprocess.run(['zsh', '-c', command], stdout=subprocess.PIPE, text=True)\n",
    "    return result.stdout.strip()\n",
    "\n",
    "api_key = get_env_from_zshrc('FW_CLI_API_KEY')\n",
    "# Connect to your Flywheel instance\n",
    "\n",
    "fw = flywheel.Client(api_key=api_key)\n",
    "display(f\"User: {fw.get_current_user().firstname} {fw.get_current_user().lastname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#api_key = os.environ.get('FW_CLI_API_KEY')\n",
    "fw = flywheel.Client(api_key=api_key)\n",
    "\n",
    "display(f\"User: {fw.get_current_user().id}\")\n",
    "\n",
    "project = fw.projects.find_first('label=LONISAC')\n",
    "display(f\"Project: {project.label}\")\n",
    "project = project.reload()\n",
    "#find the last run of orbit, and get the outlier output\n",
    "import logging\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "analyses = project.analyses\n",
    "\n",
    "last_run_date_orbit = None\n",
    "all_runs = []\n",
    "run_states = []\n",
    "orbit = None\n",
    "for asys in analyses:\n",
    "\n",
    "    try:\n",
    "\n",
    "        all_runs.append(asys.reload().gear_info.get('name'))\n",
    "        run_states.append(asys.reload().job.get('state'))\n",
    "\n",
    "        gear_name = asys.gear_info.get('name')\n",
    "        created_date = asys.created\n",
    "\n",
    "        if gear_name == \"orbit\":\n",
    "            last_run_date_orbit = max(last_run_date_orbit, created_date) if last_run_date_orbit else created_date\n",
    "            orbit = asys\n",
    "\n",
    "       \n",
    "    except Exception as e:\n",
    "        log.info(f'Exception caught:  {e}')\n",
    "\n",
    "last_run_date_orbit = last_run_date_orbit.strftime('%Y-%m-%d') if last_run_date_orbit else None\n",
    "print(\"Date ORBIT last ran: \", last_run_date_orbit)\n",
    "\n",
    "for f in orbit.files:\n",
    "    if f.name.endswith('outliers.csv'):\n",
    "        outlier_file = f\n",
    "        download_dir = Path(\"/Users/Hajer/unity/fw-notebooks/QC\")\n",
    "        download_dir.mkdir(parents=True,exist_ok=True)\n",
    "        download_path = os.path.join(download_dir , outlier_file.name)\n",
    "        outlier_file.download(download_path)\n",
    "        break\n",
    "\n",
    "\n",
    "df_outliers = pd.read_csv(download_path)\n",
    "df_outliers.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import skimage\n",
    "import plotly.express as px\n",
    "from skimage.transform import resize\n",
    "\n",
    "def preprocess_nifti(nifti_path, target_height=300):\n",
    "    # load data\n",
    "    data = nib.load(nifti_path).get_fdata()\n",
    "    data = np.nan_to_num(data)\n",
    "    xyz = data.shape\n",
    "\n",
    "    # Normalise entire data arrays\n",
    "    min_val, max_val = np.min(data), np.max(data)\n",
    "    if max_val > min_val:\n",
    "        data = (255 * (data - min_val) / (max_val - min_val))\n",
    "    else:\n",
    "        data = np.zeros_like(data)  # Black image if constant\n",
    "        \n",
    "    # Determine orientation and choose slices correctly\n",
    "    if 'SAG' in nifti_path.upper():\n",
    "        orientation = \"sag\"\n",
    "        plane = (1, 2)\n",
    "        ax = 0\n",
    "    elif 'COR' in nifti_path.upper():\n",
    "        orientation = \"cor\"\n",
    "        plane = (0, 2)\n",
    "        ax = 1\n",
    "    elif 'AXI' in nifti_path.upper():\n",
    "        orientation = \"axi\"\n",
    "        plane = (0, 1)\n",
    "        ax = 2\n",
    "    \n",
    "    # Resize the image to target height\n",
    "    w, h = xyz[plane[0]], xyz[plane[1]]\n",
    "    scale_factor = target_height / h\n",
    "    new_width = int(w * scale_factor)\n",
    "    \n",
    "    # Create new shape and resize\n",
    "    new_shape = list(xyz)\n",
    "    new_shape[plane[0]], new_shape[plane[1]] = new_width, target_height\n",
    "    data = skimage.transform.resize(data, new_shape, mode='constant', preserve_range=True)\n",
    "    \n",
    "    return data, plane[0], ax\n",
    "        \n",
    "def nifti_overlay_animation(native_path, seg_path, sub, outliers, target_height=300, alpha=0.4, cmap='viridis'):\n",
    "    # Load NIfTI files\n",
    "    native_img = nib.load(native_path)\n",
    "    seg_img = nib.load(seg_path)\n",
    "\n",
    "    native_data = native_img.get_fdata()\n",
    "    seg_data = seg_img.get_fdata()\n",
    "\n",
    "    # Rescale volumes to same shape (if needed)\n",
    "    if native_data.shape != seg_data.shape:\n",
    "        raise ValueError(\"Native and segmentation volumes must have the same shape.\")\n",
    "\n",
    "    # Normalize native scan to 0-255 (grayscale)\n",
    "    native_data = native_data - np.min(native_data)\n",
    "    native_data = (native_data / np.max(native_data)) * 255\n",
    "    native_data = native_data.astype(np.uint8)\n",
    "\n",
    "    # Resize to target height (optional)\n",
    "    scale = target_height / native_data.shape[0]\n",
    "    new_shape = (target_height, int(native_data.shape[1]*scale), native_data.shape[2])\n",
    "    native_data_resized = resize(native_data, new_shape, preserve_range=True, order=1).astype(np.uint8)\n",
    "    seg_data_resized = resize(seg_data, new_shape, preserve_range=True, order=0).astype(np.uint8)\n",
    "\n",
    "    # Create overlay for each slice\n",
    "    overlays = []\n",
    "    for i in range(native_data_resized.shape[2]):\n",
    "        background = np.stack([native_data_resized[:, :, i]]*3, axis=-1)  # Convert to RGB\n",
    "        mask = seg_data_resized[:, :, i]\n",
    "\n",
    "        # Apply colormap to mask\n",
    "        mask_rgb = plt.get_cmap(cmap)(mask)[:, :, :3]  # Drop alpha channel\n",
    "        mask_rgb = (mask_rgb * 255).astype(np.uint8)\n",
    "\n",
    "        # Alpha blend\n",
    "        overlay = background.copy()\n",
    "        overlay[mask > 0] = ((1 - alpha) * background[mask > 0] + alpha * mask_rgb[mask > 0]).astype(np.uint8)\n",
    "\n",
    "        overlays.append(overlay)\n",
    "\n",
    "    # Convert to 4D array for plotly\n",
    "    overlays_np = np.stack(overlays, axis=0)\n",
    "\n",
    "    # Plotly animation\n",
    "    print(f'Loading overlaid animation for {sub}, this may take a few seconds...')\n",
    "\n",
    "    # num_slices = overlays_np.shape[0]\n",
    "    # middle_index = num_slices // 2\n",
    "    # rolled_data = np.roll(overlays_np, -middle_index, axis=0)\n",
    "\n",
    "    # fig = px.imshow(rolled_data, animation_frame=0, aspect='equal')\n",
    "\n",
    "    fig = px.imshow(overlays_np, animation_frame=0, aspect='equal')\n",
    "    fig.update_layout(\n",
    "    title={\n",
    "        'text': f\"<b>⚠️ Outlier ROIs (z-score and cov):</b><br>{outliers}\",\n",
    "        'x': 0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top',\n",
    "        'font': dict(size=10)\n",
    "    },\n",
    "    autosize=True,\n",
    "    height=600,\n",
    "    coloraxis_showscale=False\n",
    ")\n",
    "    \n",
    "    fig.update_layout(autosize=True, height=600, coloraxis_showscale=False)\n",
    "    fig.update_layout(xaxis=dict(showticklabels=False), yaxis=dict(showticklabels=False))\n",
    "    fig.update_layout(dragmode=False)\n",
    "\n",
    "    # num_slices = overlays_np.shape[0]\n",
    "    # middle_index = num_slices // 2\n",
    "\n",
    "    # fig.data[0].z = overlays_np[middle_index]\n",
    "    # fig.layout.sliders[0]['active'] = middle_index\n",
    "    # fig.layout.updatemenus[0]['buttons'][0]['args'][1]['frame']['redraw'] = True\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()\n",
    "\n",
    "def get_data(sub_label, ses_label, gear, input_gear, v):\n",
    "    \n",
    "    from fw_client import FWClient\n",
    "    #api_key = os.environ.get('FW_CLI_API_KEY')\n",
    "    fw_ = FWClient(api_key=api_key)\n",
    "\n",
    "    subject = project.subjects.find_first(f'label=\"{sub_label}\"')\n",
    "    subject = subject.reload()\n",
    "    sub_label = subject.label\n",
    "    \n",
    "    session = subject.sessions.find_first(f'label=\"{ses_label}\"')\n",
    "    session = session.reload()\n",
    "    ses_label = session.label\n",
    "    print(gear, input_gear)\n",
    "\n",
    "    analyses = session.analyses\n",
    "\n",
    "    # If there are no analyses containers, we know that this gear was not run\n",
    "    if len(analyses) == 0:\n",
    "        run = 'False'\n",
    "        status = 'NA'\n",
    "        print('No analysis containers')\n",
    "    else:\n",
    "        try:\n",
    "            if input_gear.startswith(\"gambas\"):\n",
    "                gear = gear + \"-gambas\"\n",
    "                print(\"Looking for anaylyses from \", gear)\n",
    "            matches = [asys for asys in analyses if asys.label.startswith(gear) and asys.job.get('state') == \"complete\"]\n",
    "\n",
    "            print(\"Matches: \", len(matches),[asys.label for asys in matches] )\n",
    "            # If there are no matches, the gear didn't run\n",
    "            if len(matches) == 0:\n",
    "                run = 'False'\n",
    "                status = 'NA'\n",
    "                print(f\"Did not find any matched, {gear} did not run.\")\n",
    "            # If there is one match, that's our target\n",
    "            elif len(matches) == 1:\n",
    "                run = 'True'\n",
    "                #status = matches[0].job.get('state')\n",
    "                #print(status)\n",
    "                #print(\"Inputs \", matches[0])\n",
    "                print(\"All files...\", len(matches[0].files), [f.name for f in matches[0].files])\n",
    "                for file in matches[0].files:  \n",
    "                   \n",
    "                    if file.name.endswith('.nii.gz') : #or re.search(rf'{input_gear}.*\\.nii.gz', file.name):\n",
    "\n",
    "                    #if file.name.endswith('aparc+aseg.nii.gz') or file.name.endswith('synthSR.nii.gz') or re.search('ResCNN.*\\.nii.gz', file.name) or re.search('mrr_fast.*\\.nii.gz', file.name) or re.search('mrr-axireg.*\\.nii.gz', file.name) or re.search('.*\\.zip', file.name):\n",
    "                        parcellation = file\n",
    "                        print(\"Found \", file.name)\n",
    "                        if file :\n",
    "                            download_dir = Path(f'./QC/{sub_label}/{ses_label}/')\n",
    "                            download_dir.mkdir(parents=True,exist_ok=True)\n",
    "                            download_path = os.path.join(download_dir , parcellation.name)\n",
    "                            \n",
    "                            parcellation.download(download_path)\n",
    "                            print('Downloaded parcellation ',download_path)\n",
    "                            input_file = matches[0].inputs[0]\n",
    "\n",
    "                            print(\"Type of input file\", type(input_file))  # Should be something like flywheel.models.file.FileReference\n",
    "                            \n",
    "                            \n",
    "                            input_file=matches[0].inputs[0]\n",
    "                            print(f\"N inputs for {matches[0].label}\", len(matches[0].inputs), input_file.name)\n",
    "                            download_path = os.path.join(download_dir , input_file.name)\n",
    "                            fw.download_input_from_analysis(matches[0].id, input_file.name, download_path)\n",
    "                           \n",
    "                            return input_file\n",
    "\n",
    "\n",
    "\n",
    "            else:\n",
    "                last_run_date = max([asys.created for asys in matches])\n",
    "                last_run_analysis = [asys for asys in matches if asys.created == last_run_date]\n",
    "\n",
    "                # There should only be one exact match\n",
    "                last_run_analysis = last_run_analysis[0]\n",
    "\n",
    "                run = 'True'\n",
    "                #status = last_run_analysis.job.get('state')\n",
    "                print(\"All files...\", len(last_run_analysis.files), [f.name for f in last_run_analysis.files])\n",
    "                for file in last_run_analysis.files:\n",
    "                    if file.name.endswith('.nii.gz'): #or re.search(rf'{input_gear}.*\\.nii.gz', file.name):\n",
    "                    #if file.name.endswith('aparc+aseg.nii.gz') or file.name.endswith('synthSR.nii.gz') or re.search('ResCNN.*\\.nii.gz', file.name) or re.search('mrr_fast.*\\.nii.gz', file.name) or re.search('mrr-axireg.*\\.nii.gz', file.name) or re.search('.*\\.zip', file.name):\n",
    "                        parcellation = file\n",
    "                        print(\"Found \", file.name)\n",
    "                        if file :\n",
    "                            download_dir = Path(f'./QC/{sub_label}/{ses_label}/')\n",
    "                            download_dir.mkdir(parents=True,exist_ok=True)\n",
    "                            download_path = os.path.join(download_dir , parcellation.name)\n",
    "                            parcellation.download(download_path)\n",
    "                            print('Downloaded parcellation ',download_path)\n",
    "                            print(\"Type of parcellation file\" , type(parcellation))  # Should be something like flywheel.models.file.FileReference\n",
    "                            # print(vars(parcellation))\n",
    "\n",
    "                            input_file=last_run_analysis.inputs[0]\n",
    "                            print(f\"N inputs for {last_run_analysis.label}\", len(last_run_analysis.inputs), input_file.name)\n",
    "                            download_path = os.path.join(download_dir , input_file.name)\n",
    "                            #input_file_obj = fw.files.find_one(f'parent_ref.id={input_file.id},name={input_file.name}')\n",
    "                            fw.download_input_from_analysis(last_run_analysis.id, input_file.name, download_path)\n",
    "                            \n",
    "                            return input_file\n",
    "        except Exception as e:\n",
    "            print(f\"Exception caught for {sub_label} {ses_label} {parcellation.name}: \", e)\n",
    "            \n",
    "\n",
    "df_outliers = pd.read_csv(\"/Users/Hajer/unity/fw-notebooks/QC/minimorph_outliers.csv\")\n",
    "\n",
    "project.label\n",
    "# gear = \"mrr-axireg\"\n",
    "# subjects = project.subjects()\n",
    "# get_data(gear, subjects)\n",
    "for _, row in df_outliers.iterrows():\n",
    "    input_gear, v = row[\"input gear v\"].split(\"/\")[0], row[\"input gear v\"].split(\"/\")[1]\n",
    "    sub_label, ses_label = row[\"subject\"],row[\"session\"]\n",
    "    gear = \"minimorph\"\n",
    "    input_file= get_data(sub_label, ses_label, gear, input_gear, v)\n",
    "metrics =[\"Are the main brain structures correctly segmented (e.g., gray/white matter, ventricles)? (Yes/No)\",\n",
    "          \"Are there any major errors or artifacts? (No/Minor/Major)\",\n",
    "          \"Overall segmentation quality (Good/Acceptable/Poor)\", \n",
    "          \"Comments\"]\n",
    "\n",
    "\n",
    "def load_ratings(RATINGS_FILE):\n",
    "    if os.path.exists(RATINGS_FILE):\n",
    "        return pd.read_csv(RATINGS_FILE)\n",
    "    return pd.DataFrame(columns=[\"User\", \"Timestamp\", \"Project\", \"Subject\", \"Session\"] + metrics)\n",
    "    \n",
    "# Function to simplify acquisition labels\n",
    "def simplify_label(label):\n",
    "    # Initialize empty result\n",
    "    result = []\n",
    "    \n",
    "    # Check for orientation\n",
    "    if 'AXI' in label.upper():\n",
    "        result.append('AXI')\n",
    "    elif 'COR' in label.upper():\n",
    "        result.append('COR')\n",
    "    elif 'SAG' in label.upper():\n",
    "        result.append('SAG')\n",
    "        \n",
    "    elif 'Localizer' in label:\n",
    "        result.append('LOC')\n",
    "        \n",
    "    # Check for T1/T2\n",
    "    if 'T1' in label.upper():\n",
    "        result.append('T1')\n",
    "    elif 'T2' in label.upper():\n",
    "        result.append('T2')\n",
    "        \n",
    "    # Check for fast vs. standard labels\n",
    "    if 'FAST' in label.upper():\n",
    "        result.append('FAST')\n",
    "    elif 'STANDARD' in label.upper():\n",
    "        result.append('STANDARD')\n",
    "        \n",
    "    # Check for Gray_White\n",
    "    if 'Gray' in label.upper():\n",
    "        result.append('GrayWhite')\n",
    "        \n",
    "    # Return combined result or original label if no matches\n",
    "    return '_'.join(result) if result else label\n",
    "\n",
    "def save_rating(ratings_file, responses):\n",
    "    df = load_ratings(ratings_file)\n",
    "    new_entry = pd.DataFrame([responses], \n",
    "                              columns=df.columns)\n",
    "    \n",
    "    df = pd.concat([df, new_entry], ignore_index=True)\n",
    "    # df.loc[:, 'Acquisition'] = df['Acquisition'].apply(simplify_label)\n",
    "    \n",
    "    df.to_csv(ratings_file, index=False)\n",
    "    print(f\"\\nSaved rating: {ratings_file}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "def find_csv_file(directory, username):\n",
    "    username_cleaned = username.replace(\" \", \"\")\n",
    "    \n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if username_cleaned in file and file.endswith(\".csv\"):\n",
    "                return os.path.join(root, file)  # Return the first matching file found\n",
    "\n",
    "    return None  # No matching file found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username=input(f\"Enter your username: \")\n",
    "\n",
    "download_dir = \"/Users/Hajer/unity/GF Sprint25/QC\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "pbar = tqdm(df_outliers.subject, desc=\"Subjects evaluated\", position=0, leave=True)\n",
    "\n",
    "exit_ = False\n",
    "for _, row in df_outliers.iterrows():\n",
    "    input_gear, v = row[\"input gear v\"].split(\"/\")[0], row[\"input gear v\"].split(\"/\")[1]\n",
    "    sub_label, ses_label, project = row[\"subject\"],row[\"session\"], row[\"project_label\"]\n",
    "    native_scan , segmentation = None, None\n",
    "    files = os.listdir(path=f\"{download_dir}/{sub_label}/{ses_label}\")\n",
    "\n",
    "    for file in files:\n",
    "        if file.endswith('segmentation.nii.gz'):\n",
    "            segmentation_path = os.path.join(f\"{download_dir}/{sub_label}/{ses_label}\",file)\n",
    "        else:\n",
    "            native_scan_path = os.path.join(f\"{download_dir}/{sub_label}/{ses_label}\",file)\n",
    "\n",
    "    native_scan = nib.load(native_scan_path)        \n",
    "    segmentation = nib.load(segmentation_path)\n",
    "\n",
    "\n",
    "    if native_scan and segmentation: #if files are not empty\n",
    "                \n",
    "        # Resample the segmentation to match the native scan space if needed\n",
    "        resampled_segmentation = image.resample_to_img(segmentation, native_scan, interpolation=\"nearest\")\n",
    "        parc_data = resampled_segmentation.get_fdata()\n",
    "\n",
    "        # Get unique labels (excluding 0 which is background)\n",
    "        unique_labels = np.unique(parc_data)\n",
    "        unique_labels = unique_labels[unique_labels > 0]  # Remove background\n",
    "\n",
    "        # Generate distinct colors for each region\n",
    "        num_labels = len(unique_labels)\n",
    "        colors = plt.cm.tab20(np.linspace(0, 1, num_labels))  # Use tab20 colormap for categorical data\n",
    "\n",
    "        # Create a colormap that maps each label to a color\n",
    "        label_to_color = {label: colors[i] for i, label in enumerate(unique_labels)}\n",
    "        cmap_list = [label_to_color.get(label, (0, 0, 0, 0)) for label in range(int(unique_labels.max()) + 1)]  # Ensure mapping\n",
    "        cmap = mcolors.ListedColormap(cmap_list)\n",
    "\n",
    "       \n",
    "\n",
    "        #Now the user fills in the responses\n",
    "\n",
    "        responses = [username, timestamp, project, sub_label, ses_label] \n",
    "\n",
    "        # Have user input evaluation of image quality\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        df_outliers.drop(columns=['is_outlier'])\n",
    "        filtered_cols = [col for col in df_outliers.columns if (\n",
    "                                                (col.endswith(\"_zscore\") or col.endswith(\"_cov\"))\n",
    "                                                and not col.startswith(\"n_roi_outliers\")\n",
    "                                            )]\n",
    "        # 2. From those, keep only columns where the value is 1\n",
    "        outlier_rois = [col for col in filtered_cols if row[col] == 1]\n",
    "        \n",
    "        print(\"Outlier regions for this subject (using z score and cov):\")\n",
    "        print(f\"{set(outlier_rois)}\")\n",
    "        outliers = list(set(outlier_rois))\n",
    "        nifti_overlay_animation(\n",
    "            native_path=native_scan_path,\n",
    "            seg_path=segmentation_path,\n",
    "            sub=sub_label, \n",
    "            outliers=outliers,\n",
    "            target_height=300,\n",
    "            alpha=0.4,\n",
    "            cmap='magma'  # Any matplotlib colormap works\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "        for metric in metrics[:-1]:\n",
    "            response = \"\"\n",
    "            while response.upper() not in [\"YES\", \"NO\", \"ACCEPTABLE\", \"GOOD\", \"POOR\",\"MAJOR\",\"MINOR\"]:\n",
    "                print('Answer with one of the options listed in the question.\\n')\n",
    "                response =input(f\"{metric}:\")\n",
    "\n",
    "\n",
    "            # Store responses for paired image set\n",
    "            responses.append(response)\n",
    "\n",
    "        # Have user input any comments to elaborate on artefact presence\n",
    "        comments =input(f\"   Any {metrics[-1]}: \")\n",
    "        responses.append(comments)\n",
    "        # responses.append(timestamp)\n",
    "\n",
    "        print(responses)\n",
    "        ratings_file = os.path.join(download_dir,f'Parcellation_QC_{username.replace(\" \",\"\")}.csv')\n",
    "        save_rating(ratings_file,responses)\n",
    "\n",
    "        # if i < len(metrics) - 2:  # Only ask if not the last metric\n",
    "        should_continue = input(\"Do you want to continue to the next record? (Y/n): \").strip().lower()\n",
    "        if should_continue == \"n\":\n",
    "            print(\"Exiting after current response.\")\n",
    "            break\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        tqdm.write(\" \")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
