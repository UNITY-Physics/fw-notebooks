{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed8e24ba-f4a9-4cee-b1d7-823c002b65f4",
   "metadata": {},
   "source": [
    "# Notebook for batch processing (Running Gears)\n",
    "Niall Bourke  \n",
    "11-11-24   \n",
    "\n",
    "**Summary**\n",
    "This script automates the process of searching for specific MRI files in a project, preparing them for analysis, and submitting them to run using a gear. It includes error handling and organizes files into the correct format for submission, making it an efficient way to manage MRI data processing.\n",
    "\n",
    "This Python code is designed to loop through all the subjects and sessions in a project, and for each session, it will:\n",
    "- Iterate through Acquisitions: The code iterates through all the acquisitions in each session, looking for specific Nifti files that match certain criteria (e.g., 'T2', 'AXI', 'COR', 'SAG', and excluding 'Segmentation' and 'Align'). These files are then stored in a dictionary called inputs.  \n",
    "- Set Target Template: The code sets the target_template variable to 'None', which is used as the default configuration for the analysis.   \n",
    "- Run Analysis: The code then attempts to run an analysis using the gear.run() function, passing in the analysis_label, inputs, destination (the current session), tags, and a configuration dictionary that includes the target_template and prefix values.  \n",
    "- Error Handling: If there is an error running the analysis, the code will print a warning message.  \n",
    "\n",
    "\\* This example is running the MRR (Multi Resolution Reconstruction) gear, which takes AXI, COR, SAG acquisitions from Hyperfine Swoop as input to create an isotropic reconstruction   \n",
    "\n",
    "**Notebook environment setup**\n",
    "- Load required python packages\n",
    "- Check connection to project through Flywheel SDK\n",
    "- Set Gear options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe9a029-f2a5-45f1-9e70-87cbc757e355",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db4641e-595a-4de5-904b-bbe57924ac2c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setup options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309ad196-f5d2-413e-b232-599e82b5ab1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from IPython.display import display\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c33421e7-e0b5-4534-b02f-d6f92cca6993",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'User: nialljbourke@gmail.com'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Project: ISMRM_UNITY_DEMO'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(f\"User: {fw.get_current_user().id}\")\n",
    "\n",
    "project=fw_project\n",
    "display(f\"Project: {project.label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1bca786-f08e-417e-afbe-285d9c42b804",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gear =  fw.lookup('gears/mrr')\n",
    "analysis_tag = 'mrr-axireg'\n",
    " # Initialize gear_job_list\n",
    "job_list = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feea9b1f-e539-49b5-a6b2-45fa4751ac16",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Main code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45418ff3-4c54-45db-8615-3f0513a4dd91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing...  DEMO-001 2023-06-09 13_54_24\n",
      "inputs:  4_T2_AXI_Fast.nii.gz\n",
      "inputs:  6_T2_SAG_Fast.nii.gz\n",
      "inputs:  5_T2_COR_Fast.nii.gz\n",
      "Submitting Job: Check Jobs Log 2023-06-09 13_54_24\n"
     ]
    }
   ],
   "source": [
    "# Loop through all the subjects in the project\n",
    "for subject in project.subjects.iter():\n",
    "    for session in subject.sessions.iter():\n",
    "        session = session.reload()\n",
    "        print(\"Parsing... \", subject.label, session.label)\n",
    "\n",
    "        inputs = {}\n",
    "        EXCLUDE_PATTERNS = ['Segmentation', 'Align', 'Mapping']\n",
    "        INCLUDE_PATTERN = 'T2'\n",
    "        PLANE_TYPES = ['AXI', 'COR', 'SAG']\n",
    "\n",
    "        # Look at every acquisition in the session\n",
    "        for acquisition in session.acquisitions.iter():\n",
    "            acquisition = acquisition.reload()\n",
    "            for file in acquisition.files:\n",
    "                # We only want anatomical Nifti's\n",
    "                if file.type == 'nifti' and INCLUDE_PATTERN in file.name:\n",
    "                    if all(pattern not in file.name for pattern in EXCLUDE_PATTERNS):\n",
    "                        for plane in PLANE_TYPES:\n",
    "                            if plane in file.name:\n",
    "                                input_label = plane.lower()\n",
    "                                inputs[input_label] = file\n",
    "                                print(\"inputs: \", file.name)\n",
    "                                break\n",
    "\n",
    "        # Set to 'None' in order to run axi registration by default\n",
    "        target_template = 'None'\n",
    "\n",
    "        try:\n",
    "            # The destination for this analysis will be on the session\n",
    "            dest = session\n",
    "            time_fmt = '%d-%m-%Y_%H-%M-%S'\n",
    "            analysis_label = f'{analysis_tag}_{datetime.now().strftime(time_fmt)}'\n",
    "            job_id = gear.run(\n",
    "                analysis_label=analysis_label,\n",
    "                inputs=inputs,\n",
    "                destination=dest,\n",
    "                tags=[''],\n",
    "                config={\n",
    "                    \"target_template\": target_template,\n",
    "                    \"prefix\": analysis_tag,\n",
    "                }\n",
    "            )\n",
    "            job_list.append(job_id)\n",
    "            print(\"Submitting Job: Check Jobs Log\", dest.label)\n",
    "        except Exception as e:\n",
    "            print(f\"WARNING: Job cannot be sent for {dest.label}. Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726ea864-b03d-4476-bc23-df60ff3eb90e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2525f81-68c3-4288-b581-ebbc9a9f24ef",
   "metadata": {},
   "source": [
    "# Code Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c83512-32e7-43cd-8ec6-2d00b060e78b",
   "metadata": {},
   "source": [
    "### 1. Loop Through All Subjects and Sessions\n",
    "\n",
    "**Loop through all the subjects in the project**\n",
    "\n",
    "```\n",
    "for subject in project.subjects.iter():  \n",
    "    for session in subject.sessions.iter():  \n",
    "        session = session.reload()  \n",
    "        print(\"parsing... \", subject.label, session.label)  \n",
    "\n",
    "```\n",
    "\n",
    "- **project.subjects.iter():** This iterates over each subject in the project.  \n",
    "- **subject.sessions.iter():** This iterates over each session for the current subject.  \n",
    "- **session.reload():** Reloads the session data to ensure it is up-to-date.  \n",
    "- **print(\"parsing... \", subject.label, session.label):** Prints the subject and session labels to show which session is currently being processed.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e90e7e3-fe62-4920-90c8-ecf530e47e7e",
   "metadata": {},
   "source": [
    "### 2. Prepare to Identify Files\n",
    "\n",
    "`inputs = {}`\n",
    "\n",
    "- **inputs:* An empty dictionary created to store specific MRI data files that meet our criteria.\n",
    "- Resetting this distionary to empty is important to make sure a previous subjects files does not exist in it, if the current subject has no matching acquisitions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7757b3-504b-42dc-94e5-71b26b81f2bd",
   "metadata": {},
   "source": [
    "### 3. Loop Through Acquisitions in the Session\n",
    "\n",
    "```\n",
    "# Look at every acquisition in the session\n",
    "for acq in session.acquisitions.iter():\n",
    "    acq = acq.reload()\n",
    "    for file_obj in acq.files:\n",
    "        # We only want anatomical Nifti's\n",
    "```\n",
    "\n",
    "**session.acquisitions.iter():** Loops over all the acquisitions (collections of related images) within the session.  \n",
    "**acq.reload():** Reloads acquisition data to ensure it is current.  \n",
    "**file_obj:** Represents each file in the acquisition. The script checks if these files are anatomical MRI images in the NIfTI format.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e3cbaf-75ff-40fb-87a6-ab70a9443e03",
   "metadata": {},
   "source": [
    "### 4. Identify Specific NIfTI Files\n",
    "The script searches for files matching certain criteria:\n",
    "\n",
    "``` \n",
    "if file_obj.type == 'nifti' and 'T2' in file_obj.name and 'AXI' in file_obj.name and 'Segmentation' not in file_obj.name and 'Align' not in file_obj.name:\n",
    "    input_label = 'axi'\n",
    "    inputs[input_label] = file_obj\n",
    "    print(\"inputs: \", file_obj.name)\n",
    "```\n",
    "\n",
    "**file_obj.type == 'nifti':** Ensures the file is of type 'nifti', a common format for storing MRI data.  \n",
    "**'T2' in file_obj.name:** Checks if the file name contains 'T2', indicating a T2-weighted MRI.  \n",
    "**'AXI' in file_obj.name:** Searches for axial plane images ('AXI' indicates axial).  \n",
    "**Segmentation and Align Checks:** Ensures the file name does not contain 'Segmentation' or 'Align', filtering out irrelevant files.  \n",
    "**inputs[input_label] = file_obj:** Stores the matching file in the inputs dictionary, using 'axi', 'cor', or 'sag' as keys.  \n",
    "\n",
    "The same logic applies to identifying:  \n",
    "**'COR':** Coronal plane images.  \n",
    "**'SAG':** Sagittal plane images.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54a50ee-b536-4a17-933b-313c353d6a55",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5. Run the Analysis\n",
    "\n",
    "```\n",
    "try:\n",
    "    dest = session\n",
    "    time_fmt = '%d-%m-%Y_%H-%M-%S'\n",
    "    analysis_label = f'mrr_axireg_{datetime.now().strftime(time_fmt)}'\n",
    "    job_id = gear.run(analysis_label=analysis_label, inputs=inputs, destination=dest, tags=['BIDS'], config={\n",
    "        \"target_template\": target_template,\n",
    "        \"prefix\": \"mrr-axireg\",\n",
    "    })\n",
    "    job_list.append(job_id)\n",
    "    print(\"Submitting Job: Check Jobs Log\", dest.label)\n",
    "except:\n",
    "    print(\"WARNING: Job cannot be sent.. \", dest.label)\n",
    "    \n",
    "```\n",
    "\n",
    "**try-except Block:** Attempts to submit a job for analysis and handles errors gracefully.  \n",
    "**dest = session:** Sets the session as the destination where the analysis results will be stored.  \n",
    "**time_fmt:** Defines a date-time format for naming the analysis.  \n",
    "**analysis_label:** Creates a unique label for the analysis using the current date and time.   \n",
    "**gear.run(...):** Submits the job for analysis with:  \n",
    "- **analysis_label:** The label for this analysis run.  \n",
    "- **inputs:** The dictionary of identified files.  \n",
    "- **destination:** The session where the results should be saved.  \n",
    "- **tags:** Adds metadata tags, such as 'BIDS', for organization.  \n",
    "- **config:** Includes parameters like target_template and a prefix for the analysis.  \n",
    "- **job_list.append(job_id):** Adds the job ID to a list to track all submitted jobs.  \n",
    "\n",
    "**print(\"Submitting Job: Check Jobs Log\", dest.label):** Prints confirmation that the job was submitted.  \n",
    "**except Block:** If an error occurs, prints a warning message indicating the job couldn’t be sent.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c644982",
   "metadata": {},
   "source": [
    "## Utility Gear Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "079e7b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_utility_gear(gear_name, subjects, include_patterns, exclude_patterns, filetype, config_input_key,destination):\n",
    "    \n",
    "    \"\"\"Submits a job with specified gear and inputs.\n",
    "\n",
    "    Args:\n",
    "        gear_name (string): A Flywheel Gear name.\n",
    "        subjects (list): list of subjects to run the gear on\n",
    "        include_patterns (list): patterns to look for in a filename in order to include it in the analysis\n",
    "        exclude_patterns (list): patterns to look for in a filename to be EXCLUDED from the analysis\n",
    "        filetype (string): filetypes of interest for this analysis (nifti, dicom, etc)\n",
    "        config_input_key (string): the key specified in the manifest of the gear to specify inputs \n",
    "        destination (string): where to to store the output (session or acquisition) \n",
    "\n",
    "\n",
    "    Returns: \n",
    "        The function prints out the session for which the job was submitted. This can be verified using the 'Job Log' on the flywheel interface\n",
    "        \n",
    "    \"\"\"\n",
    "        \n",
    "    # Initialize gear_job_list\n",
    "    job_list =  list()\n",
    "    \n",
    "    gear =  fw.lookup(f'gears/{gear_name}')\n",
    "\n",
    "    # Loop through all the subjects in the project\n",
    "    for s in subjects: \n",
    "        subject = fw.subjects.find_first(f\"label={s}\")\n",
    "        subject = subject.reload()\n",
    "        for session in subject.sessions.iter():\n",
    "            session = session.reload()\n",
    "            print(\"Parsing... \", subject.label, session.label)\n",
    "\n",
    "            inputs = {}\n",
    "            \n",
    "\n",
    "            # Look at every acquisition in the session\n",
    "            for acquisition in session.acquisitions.iter():\n",
    "                acquisition = acquisition.reload()\n",
    "                for file in acquisition.files:\n",
    "                    #print(f'{len(acquisition.files)} found')\n",
    "                    # We only want anatomical Nifti's\n",
    "                    if file.type == filetype and  any(x not in file.name for x in include_patterns):\n",
    "                        if all(pattern not in file.name for pattern in exclude_patterns):\n",
    "                            inputs[config_input_key] = file\n",
    "                            print(\"inputs: \", file.name)\n",
    "                            try:\n",
    "                                dest = None\n",
    "                                # Specify the destination for this analysis\n",
    "                                if destination == 'session':\n",
    "                                    dest = session\n",
    "                                elif destination == 'acquisition':\n",
    "                                    dest = acquisition\n",
    "                                else:\n",
    "                                    print('Invalid destination specified')\n",
    "                                    \n",
    "                                time_fmt = '%d-%m-%Y_%H-%M-%S'\n",
    "                                \n",
    "                                job_id = gear.run(\n",
    "                                    inputs=inputs,\n",
    "                                    destination=dest,\n",
    "                                    tags=['']\n",
    "                                    # ,\n",
    "                                    # config={\n",
    "                                    #     \"target_template\": target_template,\n",
    "                                    #     \"prefix\": analysis_tag,\n",
    "                                    # }\n",
    "                                )\n",
    "                                job_list.append(job_id)\n",
    "                                print(\"Submitting Job: Check Jobs Log\", dest.label)\n",
    "                            except Exception as e:\n",
    "                                print(f\"WARNING: Job cannot be sent for {dest.label}. Error: {e}\")\n",
    "                                \n",
    "                            break\n",
    "                           \n",
    "                                    \n",
    "\n",
    "            # Set to 'None' in order to run axi registration by default\n",
    "            # target_template = 'None'\n",
    "\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db23ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gears_input_config = {'dcm2niix':'dcm2niix_input' , 'dicom-mr-classifier':'dicom'} #@decveloppers- add utility gears as needed?\n",
    "\n",
    "GEAR_NAME = 'dicom-mr-classifier'\n",
    "SUBJECTS = [\"RMD182B1_3M\"] #If you need to run this on ALL subjects in the project, pass the following value to your subject variable: [s.label for s in project.subjects.iter()]\n",
    "INCLUDE_PATTERNS = ['T2','T1','PSIF','CALIPR'] #Add patterns as needed\n",
    "EXCLUDE_PATTERNS = ['Segmentation', 'Mapping']\n",
    "FILETYPE = 'dicom'\n",
    "\n",
    "CONFIG_INPUT_KEY = gears_input_config[GEAR_NAME]\n",
    "DESTINATION = 'acquisition'\n",
    "run_utility_gear(GEAR_NAME, SUBJECTS, INCLUDE_PATTERNS, EXCLUDE_PATTERNS, FILETYPE, CONFIG_INPUT_KEY, DESTINATION)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea83761",
   "metadata": {},
   "source": [
    "## Analysis Gear Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09360f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_analysis_gear(gear_name, subjects, include_patterns, exclude_patterns, plane_types, filetype, analysis_tag):\n",
    "    \n",
    "    \"\"\"Submits a job with specified gear and inputs.\n",
    "    \n",
    "    Args:\n",
    "        gear_name (string): A Flywheel Gear name.\n",
    "        subjects (list): list of subjects to run the gear on\n",
    "        include_patterns (list): patterns to look for in a filename in order to include it in the analysis\n",
    "        exclude_patterns (list): patterns to look for in a filename to be EXCLUDED from the analysis\n",
    "        plane_types (list): planes to include in the analysis (AXI, SAG, COR)\n",
    "        filetype (string): filetypes of interest for this analysis (nifti, dicom, etc)\n",
    "        analysis_tag (string): metadata tags for the analysis, such as 'BIDS' for organization.\n",
    "        \n",
    "    Returns: \n",
    "        The function prints out the session for which the job was submitted. This can be verified using the 'Job Log' on the flywheel interface\n",
    "        \n",
    "    \"\"\"\n",
    "    # Initialize gear_job_list\n",
    "    job_list = list()\n",
    "    \n",
    "    gear =  fw.lookup(f'gears/{gear_name}')\n",
    "\n",
    "    # Loop through all the subjects in the projec\n",
    "    for s in subjects: \n",
    "        subject = fw.subjects.find_first(f\"label={s}\")\n",
    "        subject = subject.reload()\n",
    "        for session in subject.sessions.iter():\n",
    "            session = session.reload()\n",
    "            print(\"Parsing... \", subject.label, session.label)\n",
    "\n",
    "            inputs = {}\n",
    "            \n",
    "\n",
    "            # Look at every acquisition in the session\n",
    "            for acquisition in session.acquisitions.iter():\n",
    "                acquisition = acquisition.reload()\n",
    "                for file in acquisition.files:\n",
    "                    #print(f'{len(acquisition.files)} found')\n",
    "                    # We only want anatomical Nifti's\n",
    "                    if file.type == filetype and any(x not in file.name for x in include_patterns):\n",
    "                        if all(pattern not in file.name for pattern in exclude_patterns):\n",
    "                            for plane in plane_types:\n",
    "                                if plane in file.name:\n",
    "                                    input_label = plane.lower()\n",
    "                                    inputs[input_label] = file\n",
    "                                    print(\"inputs: \", file.name)\n",
    "                                    break\n",
    "\n",
    "            # Set to 'None' in order to run axi registration by default\n",
    "            target_template = 'None'\n",
    "\n",
    "            try:\n",
    "                # The destination for this analysis will be on the session\n",
    "                dest = session\n",
    "                time_fmt = '%d-%m-%Y_%H-%M-%S'\n",
    "                analysis_label = f'{analysis_tag}_{datetime.now().strftime(time_fmt)}'\n",
    "                job_id = gear.run(\n",
    "                    analysis_label=analysis_label,\n",
    "                    inputs=inputs,\n",
    "                    destination=dest,\n",
    "                    tags=[''],\n",
    "                    config={\n",
    "                        \"target_template\": target_template,\n",
    "                        \"prefix\": analysis_tag,\n",
    "                    }\n",
    "                )\n",
    "                job_list.append(job_id)\n",
    "                print(\"Submitting Job: Check Jobs Log\", dest.label)\n",
    "            except Exception as e:\n",
    "                print(f\"WARNING: Job cannot be sent for {dest.label}. Error: {e}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d215c329",
   "metadata": {},
   "outputs": [],
   "source": [
    "## How to use the run_analysis_gear function\n",
    "\n",
    "GEAR_NAME = 'mrr'\n",
    "SUBJECTS = [\"RMD283B1_3M\"] #If you need to run this on ALL subjects in the project, pass the following value to your subject variable: [s.label for s in project.subjects.iter()]\n",
    "INCLUDE_PATTERNS = 'T2' #Add patterns as needed\n",
    "EXCLUDE_PATTERNS = ['Segmentation', 'Align', 'Mapping']\n",
    "PLANE_TYPES = ['AXI', 'COR', 'SAG']\n",
    "FILETYPE = 'nifti'\n",
    "ANALYSIS_TAG =  'mrr-axireg'\n",
    "\n",
    "run_analysis_gear(GEAR_NAME, SUBJECTS, INCLUDE_PATTERNS, EXCLUDE_PATTERNS, PLANE_TYPES, FILETYPE, ANALYSIS_TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585ad1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_recon_all():\n",
    "    gear =  fw.lookup('gears/recon-all-clinical')\n",
    "    analysis_tag = 'recon-all-clinical'\n",
    "    # Initialize gear_job_list\n",
    "    job_list = list()\n",
    "\n",
    "    for subject in project.subjects.iter():\n",
    "        subject = subject.reload()\n",
    "        for session in subject.sessions.iter():\n",
    "            session = session.reload()\n",
    "            \n",
    "            print(\"Parsing... \", subject.label, session.label)\n",
    "\n",
    "            inputs = {}\n",
    "\n",
    "            analyses = session.analyses\n",
    "\n",
    "            # If there are no analyses containers, we know that this gear was not run\n",
    "            if len(analyses) == 0:\n",
    "                run = 'False'\n",
    "                status = 'NA'\n",
    "                print('No analysis containers')\n",
    "            else:\n",
    "\n",
    "                matches = [asys for asys in analyses if asys.gear_info.get('name') == 'mrr']\n",
    "\n",
    "                # If there are no matches, the gear didn't run\n",
    "                if len(matches) == 0:\n",
    "                    run = 'False'\n",
    "                    status = 'NA'\n",
    "                # If there is one match, that's our target\n",
    "                elif len(matches) == 1:\n",
    "                    run = 'True'\n",
    "                    status = matches[0].job.get('state')\n",
    "                    print(status)\n",
    "\n",
    "                    for file in matches[0].files:  \n",
    "                        print(file.name)\n",
    "                        if re.search('mrr-axireg.*\\.nii.gz', file.name):\n",
    "                            inputfile = file\n",
    "                            print(inputfile)\n",
    "                else:\n",
    "                        last_run_date = max([asys.created for asys in matches])\n",
    "                        last_run_analysis = [asys for asys in matches if asys.created == last_run_date]\n",
    "\n",
    "                        # There should only be one exact match\n",
    "                        last_run_analysis = last_run_analysis[0]\n",
    "\n",
    "                        run = 'True'\n",
    "                        status = last_run_analysis.job.get('state')\n",
    "\n",
    "                        for file in last_run_analysis.files:\n",
    "                            if re.search('mrr-axireg.*\\.nii.gz', file.name):\n",
    "                                inputfile = file\n",
    "                                print(inputfile.name)\n",
    "\n",
    "                if inputfile:\n",
    "                    inputs[\"input\"]= inputfile\n",
    "\n",
    "                    try:\n",
    "                        # The destination for this analysis will be on the session\n",
    "                        target_template = 'None'\n",
    "                        dest = session\n",
    "                        time_fmt = '%d-%m-%Y_%H-%M-%S'\n",
    "                        analysis_label = f'{analysis_tag}_{datetime.now().strftime(time_fmt)}'\n",
    "                        job_id = gear.run(\n",
    "                            analysis_label=analysis_label,\n",
    "                            inputs=inputs,\n",
    "                            destination=dest,\n",
    "                            tags=[''],\n",
    "                            config={\n",
    "                                \"target_template\": target_template,\n",
    "                                \"prefix\": analysis_tag,\n",
    "                            }\n",
    "                        )\n",
    "                        job_list.append(job_id)\n",
    "                        print(\"Submitting Job: Check Jobs Log\", dest.label)\n",
    "                    except Exception as e:\n",
    "                        print(f\"WARNING: Job cannot be sent for {dest.label}. Error: {e}\")\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5961de8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4919cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".unityEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
